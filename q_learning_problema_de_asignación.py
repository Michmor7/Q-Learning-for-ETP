# -*- coding: utf-8 -*-
"""Q Learning - Problema de asignación.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DcrWPeeSsjO09j2ASlHls0UOBd2BwaYu

TODO:
  - Implementar la asignación de equipos
  - Poner un presupuesto general para hacer recompensas positivas
"""

from google.colab import drive
from google.colab import files
import numpy as np
import pandas as pd
from matplotlib import pyplot as plt
#import itertools
import random

# Cargamos la carpeta de drive para acceder y guardar archivos en drive
drive.mount('/content/drive')

# Cargar requerimientos de personal
load = files.upload()
req_personal = pd.read_csv("REQ_PERS_PYTHON.csv")

# Cargar costos de asignación
load = files.upload()
costos_asig = pd.read_csv("COSTOS_ASIG_PYTHON.csv")

costos_asig = costos_asig.round() # Redondeo los valores de costos_asig

#req_personal  (dia, hora, requerimiento)

"""# Configuración inicial"""

# Definición de constantes
NUM_EMPLEADOS = 18
NUM_EQUIPOS = 9 # Posteriormente usaremos este dato como num máximo de empleados asignados al mismo tiempo
NUM_HORAS = 24
NUM_DIAS = 92  # Se recomienda variar unicamente el número de días para modificar el criterio de parada

CRITERIO_PARADA = (NUM_DIAS*NUM_HORAS) - 1  # Criterio de parada: numero de registros del df 'req_personal' a recorrer,  -1 valor, porque el siguiente valor alcanza a tomar los datos datos del día siguiente y eso no nos interesa
#CRITERIO_PARADA = len(req_personal)   # Si se quiere recorrer todo el df este será el criterio de parada

# Definición de hiperparámetros (Mejores resultados)
ALPHA = 0.1  # Tasa de aprendizaje
GAMMA = 0.8  # Factor de descuento          (se atreve a cambiar más con valores pequeños)
EPSILON = 0.2  # Probabilidad de exploración
EPISODIOS = 1200  # Número de episodios de entrenamiento

"""
  Se penalizará al algoritmo cuando:
    - No tenga ningun empleado disponible para asignar (Implementado)
    - Asigne más de 12 horas extras semanales a un empleado (Sin implementar)
"""
PENALIZACION = -1000000 # Valor de recompensa al penalizar errores del algoritmo

"""#Definición del Entorno

Variables globales del entorno
"""

empleados_disponibles = None
empleados_asignados = None
empleados_en_descanso = None

# Iniciando parámetros del entorno
indice_df = 0
costo_acumulado = 0
pasos = 0

# Criterio de parada
es_final = False

# Estado inicial de prueba
estado_inicial = None

# Creo el dataframe que usaré para almacenar los horarios
asign_empleados = None

contador_10h = 0
contador_11h = 0
contador_12h = 0

penalizado = False

lista_penalizaciones = None

"""Función para reiniciar el entorno y sus variables globales adquieran el valor inicial para ejecutarlo desde el principio"""

# Función para resetear en entorno
def reiniciar():
  """
  Función para reiniciar el entorno y sus variables globales adquieran el valor
  inicial para ejecutarlo desde el principio
  """
  # Listas de gestión
  global empleados_disponibles
  global empleados_asignados
  global empleados_en_descanso
  # Variables de entorno
  global indice_df
  global costo_acumulado
  global pasos
  global es_final
  # Estado desde donde se parte la ejecución del entorno
  global estado_inicial
  # Datafram
  global asign_empleados
  # Contadores
  global contador_10h
  global contador_11h
  global contador_12h
  global penalizado
  global lista_penalizaciones

  # Crear lista de empleados
  lista_empleados = []
  for index in range(NUM_EMPLEADOS):
    lista_empleados.append(
        {'codigo': index, 'horas_asignadas': 0, 'horas_descanso': 0, 'horas_extras_trabajadas': 0, 'dias_laborados': 0, 'horas_tabajadas': 0}
    )

  # Crear listas de apoyo para la gestion de los empleados
  empleados_disponibles = lista_empleados[:]
  empleados_asignados = []
  empleados_en_descanso = []

  # Iniciando parámetros del entorno
  indice_df = 0
  costo_acumulado = 0
  pasos = 0

  # Criterio de parada
  es_final = False

  # Estado inicial de prueba
  estado_inicial = (req_personal.iloc[indice_df]['Dia'], req_personal.iloc[indice_df]['Hora'], req_personal.iloc[indice_df]['Personal_minimo'])

  # Creo el dataframe que usaré para almacenar los horarios
  asign_empleados = None
  asign_empleados = pd.DataFrame()

  asign_empleados['dia'] = costos_asig['Dia']
  asign_empleados['hora'] = costos_asig['Hora']

  array = np.zeros(len(req_personal), dtype=int)
  for index in range(NUM_EMPLEADOS):
    asign_empleados[f'empleado_{index+1}'] = array

  contador_10h = 0
  contador_11h = 0
  contador_12h = 0

  penalizado = False

  lista_penalizaciones = []

  return estado_inicial

"""Función para ejecutar el entorno"""

# 3. Definición del entorno
def paso(accion):
  """
    Esta función raliza la ejecución de nuestro entorno de acuerdo a la acción
    que defina el agente.

    Retornará el estado actual en el que se encuentra el entorno tras el paso de
    ejecución y la recompensa asociada a este paso.
  """

  """
      0. Llamamos las var globales que usaremos dentro del entorno y definimos
          otras variables iniciales.
  """
  # Listas de gestión
  global empleados_disponibles
  global empleados_asignados
  global empleados_en_descanso
  # Variables de entorno
  global indice_df
  global costo_acumulado
  global pasos
  global es_final
  # Estado desde donde se parte la ejecución del entorno
  global estado_inicial
  # Contadores
  global contador_10h
  global contador_11h
  global contador_12h
  # Variable bandera
  global penalizado
  # df
  global asign_empleados
  global lista_penalizaciones

  recompensa = 0
  siguiente_hora = False  # Var bandera que me ayuda a avanzar de hora laboral


  """
      1. Asignación de un empleado
          - Si no hay empleados disponibles para la asignación se penaliza
            fuertemente el algoritmo, pues indica que ha habido una muy mala
            gestión en las asignaciones.
          - En dado caso que se intente asignar horas extra y ningun empleado
            tenga cupo, se retornará al final de la func el mismo estado, con
            recompensa 0, para que avance la ejecución hasta que se asigne una
            jornada normal.
  """
  # Verificamos si hay demanda de empleados
  if estado_inicial[2] : # Comparo los empleados ya asignados con los empleados requeridos, estado_actual[2] indica la demanda para ese estado

    if len(empleados_disponibles) > 0:  # Verificamos si hay empleados disponibles

      for empleado in empleados_disponibles:  # Saltamos aquellos empleados disponibles que ya tengan sus horas extras semanales cumplidas

        if (accion == 1) and (empleado['horas_extras_trabajadas']+1 > 12):
          #print(f'No se puede asignar 1a hora extra a {empleado}')
          continue
        elif (accion == 2) and (empleado['horas_extras_trabajadas']+2 > 12):
          #print(f'No se puede asignar 2 horas extras a {empleado}')
          continue

        empleado['horas_asignadas'] = 10 # Incluyen un receso de 2 horas a la mitad de jornada
        empleado['horas_descanso'] = 14 # Para estos valores se pueden definir constantes
        recompensa = -(costos_asig.iloc[indice_df]['Costo_hora_normal']*(empleado['horas_asignadas']-2))

        if accion == 0:
          contador_10h += 1

        if accion == 1: # Se asigna una hora extra
          contador_11h += 1
          empleado['horas_extras_trabajadas'] += 1
          empleado['horas_asignadas'] += 1
          empleado['horas_descanso'] -= 1
          recompensa -= (costos_asig.iloc[indice_df]['Costo_hora_adicional'])

        elif accion == 2: # Se asignan dos horas extra
          contador_12h += 1
          empleado['horas_extras_trabajadas'] += 2
          empleado['horas_asignadas'] += 2
          empleado['horas_descanso'] -= 2
          recompensa -= (2*costos_asig.iloc[indice_df]['Costo_hora_adicional'])

        empleado['dias_laborados'] += 1
        empleado['horas_tabajadas'] += empleado['horas_asignadas']  # Acumulo todas las horas trabajadas por el empleado

        empleados_disponibles.remove(empleado)
        empleados_asignados.append(empleado)

        asign_empleados[f'empleado_{ empleado["codigo"]+1 }'][indice_df : indice_df+empleado['horas_asignadas']] = 1  # Registramos las horas asignadas en el Dataframe 'asign_empleados'
        #print(f'Empleado {empleado["codigo"]} asignado con turno de {empleado["horas_asignadas"]} horas')
        break

    else:
      if len(empleados_asignados) == 0:
        texto = f'PENALIZACIÓN ALTA!!!: Nadie atiende la estación - No hay ni empleados disponibles ni empleados asignados'
        print(texto)
        lista_penalizaciones.append(texto)
        recompensa = PENALIZACION
        siguiente_hora = True
        penalizado = True
      else:
        texto = f'PENALIZACIÓN MODERADA!!!: No hay empleados disponibles para cumplir la damanda (Empleados asignados: {len(empleados_asignados)} - Demanada: {len(empleados_asignados)+estado_inicial[2]})'
        print(texto)
        lista_penalizaciones.append(texto)
        recompensa = -(costos_asig.iloc[indice_df]['Costo_hora_adicional']) * estado_inicial[2] # Es aprox el precio de una hora extra multiplicado por los empleado faltantes para suplir la demanda
        siguiente_hora = True
        penalizado = True

  else: # Si no hay demanda de nuevos empleados
    siguiente_hora = True


  """
    2. Comportamiento al pasar a la siguiente hora laboral
  """
  if siguiente_hora:
    indice_df += 1  # Aumentamos un paso en el dataframe para la siguiente hora

    # Restamos una hora de descanso a los empleados en descanso
    if empleados_en_descanso:
      index = 0
      while True: # Usé un ciclo while porque al usar un for y remover elementos de empleados_en_descanso se saltaba el orden
        if index >= len(empleados_en_descanso): break   # Sí ya no hay más empleados a evaluar se sale del ciclo

        empleados_en_descanso[index]['horas_descanso'] -= 1
        if empleados_en_descanso[index]['horas_descanso'] == 0:
          empleados_disponibles.insert(0,empleados_en_descanso[index])
          empleados_en_descanso.remove(empleados_en_descanso[index])
          continue
        index += 1

    # Restamos una hora laborada a todos los empleados asignados
    if empleados_asignados:
      index = 0
      while True: # Usé un ciclo while porque al usar un for y remover elementos de empleados_asignados se saltaba el orden
        if index >= len(empleados_asignados): break     # Sí ya no hay más empleados a evaluar se sale del ciclo

        empleados_asignados[index]['horas_asignadas'] -= 1
        if empleados_asignados[index]['horas_asignadas'] == 0:  # Si termina su jornada laboral pasa a descansar

          if empleados_asignados[index]['dias_laborados'] == 6: # Si pasa a descansar y ha laborado 6 días tiene 14 horas más de descanso y se reinician las horas extra acumuladas y los dias laborados
            empleados_asignados[index]['dias_laborados'] = 0
            empleados_asignados[index]['horas_extras_trabajadas'] = 0
            empleados_asignados[index]['horas_descanso'] += 13


          empleados_en_descanso.append(empleados_asignados[index])
          empleados_asignados.remove(empleados_asignados[index])
          continue
        index += 1


  """
      3. Actualización del nuevo estado
  """
  # Calculamos la demanda para el siguiente estado
  demanda_emp = req_personal.iloc[indice_df]['Personal_minimo']-len(empleados_asignados)
  if demanda_emp < 0: demanda_emp = 0   # Si la demanda en esa hora se hace menor a los empleados asignados se redondea a 0 para que no genere error

  estado_actual = (req_personal.iloc[indice_df]['Dia'], req_personal.iloc[indice_df]['Hora'], demanda_emp)
  costo_acumulado += recompensa
  pasos += 1

  #print(f'estado_inicial:{estado_inicial} / estado_actual:{estado_actual} /  recompensa: {recompensa}   ---   indice_df: {indice_df}  costo_acumulado: {costo_acumulado}  pasos: {pasos}')
  estado_inicial = estado_actual  # Definimos el estado al que llegamos como el estado del que partiremos en la proxima iteración

  """
      4. Criterio de parada
  """
  if indice_df >= CRITERIO_PARADA:  # es_final True cuando se recorra todo el df de requerimiento de personal
    es_final = True

  return estado_actual, recompensa, es_final

"""Ejecutamos el entorno"""

# Reinicio el entorno para empezar desde 0
reiniciar()

"""for _ in range(5):
  _,_,_ = paso(2)

# Ejecuto un paso de simulación en el entorno de acuerdo a las acciones (recuerda que son 3 acciones 0(8h), 1(9h), 2(10h))
nuevo_estado, recompensa, es_final = paso(2)
#print(f'\nEstado actual: {nuevo_estado}   Recompensa: {recompensa}   Es final?: {es_final}')

Monitoreamos la asignación de empleados

print('Empleados asignados:')
for empleado in empleados_asignados:
  print(f'  {empleado}')

print('Empleados en descanso:')
for empleado in empleados_en_descanso:
  print(f'  {empleado}')

print('Empleados disponibles:')
for empleado in empleados_disponibles:
  print(f'  {empleado}')

asign_empleados

# Q-learning
"""

def ajustar_indices(estado):
  """
  Se usa esta función porque en el q-table los indices empiezan desde 0 para
  horas y dias pero en los DF no, se debe hacer ese ajuste

  Estructura estado: (dia, hora, empleados_requeridos)
  """
  nuevo_estado = (estado[0]-1, estado[1]-1, estado[2])
  return nuevo_estado

# Matriz Q correspondiente el entrenamiento
"""
  La matriz Q va a ir almacenando los respectivos valores de acción para cada
  estado, por ello debemos mapear todos los posibles estados y a cada uno de
  ellos asignarle 3 espacios adicionales, para las 3 posibles acciones que se
  pueden realizan en el entorno

  NUM_EQUIPOS lo estoy usando para definir la cantidad de empleados requeridos
  en un momento dado, son 10 porque el máximo puede ser 9 y el min 0 (no hay
  requerimiento)
"""
q_table = np.random.uniform(low= -1, high= 1, size= [NUM_DIAS, NUM_HORAS, NUM_EQUIPOS+1, 3])

# Aquí almacenaremos las recompensas por simulación
lista_mejores_resultados = []
lista_total_costo = []


"""
    Ejecución del algoritmo Q Learning
"""
for episodio in range(EPISODIOS):
  print(f'Episodio {episodio}')
  estado = reiniciar()  # Genera un nuevo estado cada que empecemos una nueva simulación, almacena el estado inicial
  estado = ajustar_indices(estado)

  final = False  # Variable para detener la simulación
  recompensa_total = 0  # Valor de recompensa total de la simulación

  # Ejecutamos la simulación
  step = 0

  while not final:
    """
      Definimos la acción a tomar, 20% de las veces será aleatorioa, el resto
      tomará la acción que tenga mayor probabilidad de éxito según la q_table
    """

    if random.random() > EPSILON:   # Se define si se explota lo aprendido o explora nuevas acciones
      accion = np.argmax(q_table[estado])  # Retorna el indice de la probabilidad más alta para un determinado estado
    else:
      accion = random.randint(0, 2)   # Escoge uno de entre tres posibles estados

    #accion = 0

    nuevo_estado, recompensa, final = paso(accion)  # Con la acción que definimos ejecutamos la simulación (un paso), nos retornará esos parámetros autodocumentados, el estado lo almacena internamente el objeto 'env'
    nuevo_estado = ajustar_indices(nuevo_estado)

    # Calculamos la probabilidad de éxito para el paso actual según la acción escogida (Es la ecuación de Bellman) (RECUERDE que la var 'estado' es una tupla de dos valores)
    q_table[estado][accion] = q_table[estado][accion] + ALPHA * (recompensa + GAMMA * np.max(q_table[nuevo_estado]) - q_table[estado][accion])

    estado = nuevo_estado  # Actualizamos el estado (para consultar q_table, porque env lo actualiza automaticamente en la simulación)
    recompensa_total += recompensa
    step += 1

  #lista_recompensas.append(recompensa_total)
  print(f'Recompensas acumuladas: {recompensa_total} - Costo acumulado: {costo_acumulado}    /       Steps: {step}  -  Pasos:{pasos}         /     Num 10h: {contador_10h}, Num 11h: {contador_11h}, Num 12h: {contador_12h}     --- Penalizado?: {penalizado}\n')
  #lista_costo
  lista_total_costo.append(recompensa_total)


  # Guardo los mejores resultados
  if len(lista_mejores_resultados) == 0:               # Si no hay elementos en los mejores resultados
    lista_mejores_resultados.append([recompensa_total, episodio, asign_empleados.copy(), lista_penalizaciones])

  elif lista_mejores_resultados[0][0] < recompensa_total: # Si hay un resultado mejor se añade
    lista_mejores_resultados.append([recompensa_total, episodio, asign_empleados.copy(), lista_penalizaciones])
    lista_mejores_resultados.sort(reverse=True)

    if len(lista_mejores_resultados) > 5:   # Si hay más de 5 elementos se elimina uno
      del(lista_mejores_resultados[-1][2])
      lista_mejores_resultados.pop()

  """if episodio == 300:
    GAMMA = 0.6
  elif episodio == 600:
    GAMMA = 0.8
    EPSILON = 0.2  # Reducimos EPSILON a la mitad para disminuir el nivel exploratorio
  elif episodio == 900:
    GAMMA = 0.9
    EPSILON = 0.1  # Reducimos EPSILON a la mitad para disminuir el nivel exploratorio"""

"""Vemos el data frame de asignaciones"""

for i in range(len(lista_mejores_resultados)):
  #Esta línea era para verificar que los df sean diferentes entre sí
  #print(f'{lista_mejores_resultados[i][0]}, {lista_mejores_resultados[i][1]}, {lista_mejores_resultados[0][2].equals(lista_mejores_resultados[i][2])}')
  print(f'{lista_mejores_resultados[i][0]}, {lista_mejores_resultados[i][1]}')

print(lista_mejores_resultados[0][3])

df_asign_empleados = lista_mejores_resultados[0][2]
df_asign_empleados

"""#Gráfica resultados generales

"""

lista_total_costo[0]

plt.plot(lista_total_costo/lista_mejores_resultados[0][0])
plt.figure(figsize=(100,60))
plt.show()

"""- Para mejorar el entrenamiento se podría definir un presupuesto general para que las recompensas sean positivas
  - Hacer una gráfica comparativa entre recompensa y episodios para ver si la falta de divergencia se debe a sobre entrenamiento (tendría que encontrar un punto mínimo para establecer las epocas)

# Guardar DataFrame de asignación de empleados y Q-table con los pesos de entrenamiento
"""

"""
  Guardamos el df de asignación de empleados
"""
# Guarda datos en CSV:
df_asign_empleados.to_csv('/content/drive/MyDrive/asign_empleados.csv', header=False, index=False)

"""
  Guardamos el q_table producto del entrenamiento realizado, antes debemos hacer reshape de q_table porque no se pueden
  guardar arrays de más de 2 dimensiones
"""
# NUM_DIAS, NUM_HORAS, NUM_EQUIPOS+1, 3
q_table_reshape = q_table.reshape(NUM_DIAS, (NUM_HORAS)*(NUM_EQUIPOS+1)*3)
np.savetxt('/content/drive/MyDrive/q_table.txt', q_table_reshape)

"""Cargo nuevamente el q-table para verificar que sea exactamente el mismo que el original"""

"""
  Cargamos el q_table producto del entrenamiento realizado, antes debemos hacer reshape de q_table porque no se pueden
  guardar arrays de más de 2 dimensiones
"""

q_table_back_2d = np.loadtxt("/content/drive/MyDrive/q_table.txt")
q_table_back = q_table_back_2d.reshape(NUM_DIAS, NUM_HORAS, NUM_EQUIPOS+1, 3)

if (q_table_back == q_table).all():
  print("Los elementos guardados del q_table son los mismos que el original")
else:
  print("Error! elementos diferentes en el q_table almacenado")